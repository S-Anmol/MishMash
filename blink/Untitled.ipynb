{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caliberation in Progress!\n",
      "Caliberation Complete!\n",
      "Current SPF (seconds per frame) is 34.26 ms\n",
      "drowsy limit: 29.190604730650936, false blink limit: 4.37859070959764\n"
     ]
    }
   ],
   "source": [
    "import dlib\n",
    "import sys\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.spatial import distance as dist\n",
    "from threading import Thread\n",
    "import playsound\n",
    "import queue\n",
    "import face_recognition\n",
    "# from light_variability import adjust_gamma\n",
    "\n",
    "FACE_DOWNSAMPLE_RATIO = 1.5\n",
    "RESIZE_HEIGHT = 460\n",
    "\n",
    "thresh = 0.3\n",
    "modelPath = \"models/shape_predictor_70_face_landmarks.dat\"\n",
    "sound_path = \"alarm.wav\"\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(modelPath)\n",
    "\n",
    "leftEyeIndex = [36, 37, 38, 39, 40, 41]\n",
    "rightEyeIndex = [42, 43, 44, 45, 46, 47]\n",
    "\n",
    "blinkCount = 0\n",
    "drowsy = 0\n",
    "state = 0\n",
    "blinkTime = 0.15 #150ms\n",
    "drowsyTime = 1.0  #1200ms\n",
    "ALARM_ON = False\n",
    "GAMMA = 1.5\n",
    "num_blinks = 3\n",
    "threadStatusQ = queue.Queue()\n",
    "\n",
    "vishal_image = face_recognition.load_image_file(\"vishal.jpg\")\n",
    "vishal_face_encoding = face_recognition.face_encodings(vishal_image)[0]\n",
    "\n",
    "# Load a second sample picture and learn how to recognize it.\n",
    "# Create arrays of known face encodings and their names\n",
    "known_face_encodings = [\n",
    "    vishal_face_encoding,\n",
    "]\n",
    "known_face_names = [\n",
    "    \"Vishal\",\n",
    "]\n",
    "\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "process_this_frame = True\n",
    "\n",
    "invGamma = 1.0/GAMMA\n",
    "table = np.array([((i / 255.0) ** invGamma) * 255 for i in range(0, 256)]).astype(\"uint8\")\n",
    "\n",
    "def gamma_correction(image):\n",
    "    return cv2.LUT(image, table)\n",
    "\n",
    "def histogram_equalization(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return cv2.equalizeHist(gray) \n",
    "\n",
    "def soundAlert(path, threadStatusQ):\n",
    "    while True:\n",
    "        if not threadStatusQ.empty():\n",
    "            FINISHED = threadStatusQ.get()\n",
    "            if FINISHED:\n",
    "                break\n",
    "        playsound.playsound(path)\n",
    "\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "\n",
    "    return ear\n",
    "\n",
    "\n",
    "def checkEyeStatus(landmarks):\n",
    "    mask = np.zeros(frame.shape[:2], dtype = np.float32)\n",
    "    \n",
    "    hullLeftEye = []\n",
    "    for i in range(0, len(leftEyeIndex)):\n",
    "        hullLeftEye.append((landmarks[leftEyeIndex[i]][0], landmarks[leftEyeIndex[i]][1]))\n",
    "\n",
    "    cv2.fillConvexPoly(mask, np.int32(hullLeftEye), 255)\n",
    "\n",
    "    hullRightEye = []\n",
    "    for i in range(0, len(rightEyeIndex)):\n",
    "        hullRightEye.append((landmarks[rightEyeIndex[i]][0], landmarks[rightEyeIndex[i]][1]))\n",
    "\n",
    "\n",
    "    cv2.fillConvexPoly(mask, np.int32(hullRightEye), 255)\n",
    "\n",
    "    # lenLeftEyeX = landmarks[leftEyeIndex[3]][0] - landmarks[leftEyeIndex[0]][0]\n",
    "    # lenLeftEyeY = landmarks[leftEyeIndex[3]][1] - landmarks[leftEyeIndex[0]][1]\n",
    "\n",
    "    # lenLeftEyeSquared = (lenLeftEyeX ** 2) + (lenLeftEyeY ** 2)\n",
    "    # eyeRegionCount = cv2.countNonZero(mask)\n",
    "\n",
    "    # normalizedCount = eyeRegionCount/np.float32(lenLeftEyeSquared)\n",
    "\n",
    "    #############################################################################\n",
    "    leftEAR = eye_aspect_ratio(hullLeftEye)\n",
    "    rightEAR = eye_aspect_ratio(hullRightEye)\n",
    "\n",
    "    ear = (leftEAR + rightEAR) / 2.0\n",
    "    #############################################################################\n",
    "\n",
    "    eyeStatus = 1          # 1 -> Open, 0 -> closed\n",
    "    if (ear < thresh):\n",
    "        eyeStatus = 0\n",
    "\n",
    "    return eyeStatus  \n",
    "\n",
    "def checkBlinkStatus(eyeStatus):\n",
    "    global state, blinkCount, drowsy\n",
    "\n",
    "    if blinkCount >= num_blinks:\n",
    "    \treturn True\n",
    "\n",
    "    if(state >= 0 and state <= falseBlinkLimit):\n",
    "        if(eyeStatus):\n",
    "            state = 0\n",
    "\n",
    "        else:\n",
    "            state += 1\n",
    "\n",
    "    elif(state >= falseBlinkLimit and state < drowsyLimit):\n",
    "        if(eyeStatus):\n",
    "            blinkCount += 1 \n",
    "            state = 0\n",
    "\n",
    "        else:\n",
    "            state += 1\n",
    "\n",
    "\n",
    "    else:\n",
    "        if(eyeStatus):\n",
    "            state = 0\n",
    "            drowsy = 1\n",
    "            blinkCount += 1\n",
    "\n",
    "        else:\n",
    "            drowsy = 1\n",
    "\n",
    "def getLandmarks(im):\n",
    "    imSmall = cv2.resize(im, None, \n",
    "                            fx = 1.0/FACE_DOWNSAMPLE_RATIO, \n",
    "                            fy = 1.0/FACE_DOWNSAMPLE_RATIO, \n",
    "                            interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "    rects = detector(imSmall, 0)\n",
    "    if len(rects) == 0:\n",
    "        return 0\n",
    "\n",
    "    newRect = dlib.rectangle(int(rects[0].left() * FACE_DOWNSAMPLE_RATIO),\n",
    "                            int(rects[0].top() * FACE_DOWNSAMPLE_RATIO),\n",
    "                            int(rects[0].right() * FACE_DOWNSAMPLE_RATIO),\n",
    "                            int(rects[0].bottom() * FACE_DOWNSAMPLE_RATIO))\n",
    "\n",
    "    points = []\n",
    "    [points.append((p.x, p.y)) for p in predictor(im, newRect).parts()]\n",
    "    return points\n",
    "\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "for i in range(10):\n",
    "    ret, frame = capture.read()\n",
    "\n",
    "totalTime = 0.0\n",
    "validFrames = 0\n",
    "dummyFrames = 100\n",
    "\n",
    "print(\"Caliberation in Progress!\")\n",
    "while(validFrames < dummyFrames):\n",
    "    validFrames += 1\n",
    "    t = time.time()\n",
    "    ret, frame = capture.read()\n",
    "    height, width = frame.shape[:2]\n",
    "    IMAGE_RESIZE = np.float32(height)/RESIZE_HEIGHT\n",
    "    frame = cv2.resize(frame, None, \n",
    "                        fx = 1/IMAGE_RESIZE, \n",
    "                        fy = 1/IMAGE_RESIZE, \n",
    "                        interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "    # adjusted = gamma_correction(frame)\n",
    "    adjusted = histogram_equalization(frame)\n",
    "\n",
    "    landmarks = getLandmarks(adjusted)\n",
    "    timeLandmarks = time.time() - t\n",
    "\n",
    "    if landmarks == 0:\n",
    "        validFrames -= 1\n",
    "        cv2.putText(frame, \"Unable to detect face, Please check proper lighting\", (10, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "        cv2.putText(frame, \"or decrease FACE_DOWNSAMPLE_RATIO\", (10, 50), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "        cv2.imshow(\"Video\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            sys.exit()\n",
    "\n",
    "    else:\n",
    "        totalTime += timeLandmarks\n",
    "        # cv2.putText(frame, \"Caliberation in Progress\", (200, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "        # cv2.imshow(\"Blink Detection Demo\", frame)\n",
    "        \n",
    "    # if cv2.waitKey(1) & 0xFF == 27:\n",
    "    #         sys.exit()\n",
    "\n",
    "print(\"Caliberation Complete!\")\n",
    "\n",
    "spf = totalTime/dummyFrames\n",
    "print(\"Current SPF (seconds per frame) is {:.2f} ms\".format(spf * 1000))\n",
    "\n",
    "drowsyLimit = drowsyTime/spf\n",
    "falseBlinkLimit = blinkTime/spf\n",
    "print(\"drowsy limit: {}, false blink limit: {}\".format(drowsyLimit, falseBlinkLimit))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    vid_writer = cv2.VideoWriter('output-low-light-2.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 15, (frame.shape[1],frame.shape[0]))\n",
    "    for _ in range(200):\n",
    "        try:\n",
    "            t = time.time()\n",
    "            ret, frame = capture.read()\n",
    "            height, width = frame.shape[:2]\n",
    "            IMAGE_RESIZE = np.float32(height)/RESIZE_HEIGHT\n",
    "            frame = cv2.resize(frame, None, \n",
    "                                fx = 1/IMAGE_RESIZE, \n",
    "                                fy = 1/IMAGE_RESIZE, \n",
    "                                interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "            # adjusted = gamma_correction(frame)\n",
    "            adjusted = histogram_equalization(frame)\n",
    "\n",
    "            landmarks = getLandmarks(adjusted)\n",
    "            if landmarks == 0:\n",
    "                validFrames -= 1\n",
    "                cv2.putText(frame, \"Unable to detect face, Please check proper lighting\", (10, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                cv2.putText(frame, \"or decrease FACE_DOWNSAMPLE_RATIO\", (10, 50), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                cv2.imshow(\"Video\", frame)\n",
    "                if cv2.waitKey(1) & 0xFF == 27:\n",
    "                    break\n",
    "                continue\n",
    "\n",
    "            eyeStatus = checkEyeStatus(landmarks)\n",
    "            res = checkBlinkStatus(eyeStatus)\n",
    "\n",
    "            if res == True:\n",
    "            \tcv2.putText(frame, \"Transaction processed\", (10, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "            for i in range(0, len(leftEyeIndex)):\n",
    "                cv2.circle(frame, (landmarks[leftEyeIndex[i]][0], landmarks[leftEyeIndex[i]][1]), 1, (0, 0, 255), -1, lineType=cv2.LINE_AA)\n",
    "\n",
    "            for i in range(0, len(rightEyeIndex)):\n",
    "                cv2.circle(frame, (landmarks[rightEyeIndex[i]][0], landmarks[rightEyeIndex[i]][1]), 1, (0, 0, 255), -1, lineType=cv2.LINE_AA)\n",
    "\n",
    "            if drowsy:\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "                cv2.putText(frame, \"Blinks : {}\".format(blinkCount), (460, 80), cv2.FONT_HERSHEY_COMPLEX, 0.8, (0,0,255), 2, cv2.LINE_AA)\n",
    "                # (0, 400)\n",
    "                ALARM_ON = False\n",
    "\n",
    "\n",
    "            k = cv2.waitKey(1) \n",
    "            if k == ord('r'):\n",
    "                state = 0\n",
    "                drowsy = 0\n",
    "                ALARM_ON = False\n",
    "                threadStatusQ.put(not ALARM_ON)\n",
    "\n",
    "            elif k == 27:\n",
    "                break\n",
    "\n",
    "            small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "            rgb_small_frame = small_frame[:, :, ::-1]\n",
    "\n",
    "            # Only process every other frame of video to save time\n",
    "            if process_this_frame:\n",
    "                # Find all the faces and face encodings in the current frame of video\n",
    "                face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "                face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "                face_names = []\n",
    "                for face_encoding in face_encodings:\n",
    "                    # See if the face is a match for the known face(s)\n",
    "                    matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "                    name = \"Unknown\"\n",
    "\n",
    "                    # # If a match was found in known_face_encodings, just use the first one.\n",
    "                    # if True in matches:\n",
    "                    #     first_match_index = matches.index(True)\n",
    "                    #     name = known_face_names[first_match_index]\n",
    "\n",
    "                    # Or instead, use the known face with the smallest distance to the new face\n",
    "                    face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "                    best_match_index = np.argmin(face_distances)\n",
    "                    if matches[best_match_index]:\n",
    "                        name = known_face_names[best_match_index]\n",
    "\n",
    "                    face_names.append(name)\n",
    "\n",
    "            process_this_frame = True if _ % 5 == 0 else False\n",
    "\n",
    "\n",
    "            # Display the results\n",
    "            for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "                # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "                top *= 4\n",
    "                right *= 4\n",
    "                bottom *= 4\n",
    "                left *= 4\n",
    "\n",
    "                # Draw a box around the face\n",
    "                cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "                # Draw a label with a name below the face\n",
    "                cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "                font = cv2.FONT_HERSHEY_DUPLEX\n",
    "                cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "            # Display the resulting image\n",
    "            cv2.imshow('Video', frame)\n",
    "\n",
    "            # Hit 'q' on the keyboard to quit!\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "            # print(\"Time taken\", time.time() - t)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
